---
title: 'Seminar 10: Descriptive statistics'
output: html_notebook
---

## 1. Data (Id is the average of the member's no. in the P&S list)

### Task: Generate your data 

This is a sample of size $n=1000$ combined from two samples of size $500$ from the normal distributions $N(\mu_k,\sigma_k^2)$ with $\mu_1 = Id$, $\sigma_1 = 3$ and $\mu_2 = -2$, $\sigma_2 = Id$

Hint: you can use $\texttt{mean = c}(\mu_1,\mu_2)$ and $\texttt{sd = c}(\sigma_1,\sigma_2)$
 

```{r}
mu_1 <- 10
mu_2 <- -2
sd_1 <- 3
sd_2 <- 10
n <- 1000
mean <- c(mu_1, mu_2)
sd <- c(sd_1, sd_2)

my.data <- rnorm(n, mean, sd)
summary(my.data)
```

## 2. Visualise the data

### Task: Draw the histogram of the data, empirical density, and empirical cdf. Comment on whether the data are close to a normal one

Let $X$ ~ $N(\mu_1, \sigma_1^2)$, $Y$ ~ $N(\mu_2, \sigma_2^2)$. Then, combining these two samples, the expected value is going to be: $E(Z) = \frac{\mu_1 + \mu_2}{2} = 4$.

$Var(Z) = E(Z^2) - {E(Z)}^2  = E(Z^2) - 16 = \frac{1}{2}(E(X^2) + E(Y^2)) - 16 = \frac{1}{2}(\mu_1^2 + \sigma_1^2 + \mu_2^2 + \sigma_2^2) - 16 = \frac{1}{2}(100 + 9 + 4 + 100) - 16 = 90.5$

```{r}
x <- my.data

# histogram
hist(x)

# superimpose normal density
h <- hist(x, plot=FALSE)
plot(h, col="grey")
xlines <-seq(min(h$breaks),max(h$breaks),length.out=1000)
lines(x = xlines,y=dnorm(xlines, 4, sqrt(90.5)) *length(x)*diff(h$breaks)[1])


# superimpose emperical density
h <- hist(x, plot=FALSE)
plot(h, col="grey")
xlines <-seq(min(h$breaks),max(h$breaks),length.out=1000)
lines(x = xlines, y=dnorm(xlines, mean(x), sd(x)) * length(x)*diff(h$breaks)[1])
```

### Plot ecdf

```{r}
Fs_1 <- ecdf(x)
# plot ecdf
plot(Fs_1)


# superimpose cdf for standard normal
par(new=TRUE)
plot(ecdf(rnorm(10e4, 0, 1)), col="red")

# calculate maximal difference
sample <- x
x <- seq(min(sample),max(sample), by = .01)
max(abs(Fs_1(x)-pnorm(x,mean = mean(sample), sd = sd(sample))))

```

## 3. Skewness and kurtosis of the data

**Skewness** is defined as
$$
  \mathsf{E} \left[\left({\frac {X-\mu }{\sigma }}\right)^{3}\right]={\frac {\mu _{3}}{\sigma ^{3}}}={\frac {\mathsf{E} \left[(X-\mu )^{3}\right]}{\ \ \ (\mathsf{E} \left[(X-\mu )^{2}\right])^{3/2}}}
$$

Positive skewness means the data have longer right tail and then the mean value $\mathsf{E}(X)$ is greater than the median

**Kurtosis** is defined as 
$$
  \mathsf{E} \left[\left({\frac {X-\mu }{\sigma }}\right)^{4}\right]={\frac {\mu _{4}}{\sigma ^{4}}}={\frac {\mathsf{E} \left[(X-\mu )^{4}\right]}{\ \ \ (\mathsf{E} \left[(X-\mu )^{2}\right])^{2}}}
$$

For a standard normal distribution, the kurtosis is $3$. If we get a larger value, then more data are concentrated ner the mean, and the empirical density is more steep


```{r} 
# your code here
k_central_moment <- function(num){
  return (sum((my.data - mean(my.data))^(num))/n)
}
# skewness
third_central_moment = k_central_moment(3)
sec_central_moment = k_central_moment(2)
skewness = third_central_moment/sec_central_moment^(3/2)
skewness
# kurtosis
four_central_moment = k_central_moment(4)
kurtosis = four_central_moment/sec_central_moment^2
kurtosis
```

## 4. Percentiles 

### For q= 0.1,...,0.9 calculate sample q-percentiles manually (i.e. by sorting the data and picking the corresponding values) and by using percentile

```{r} 
quantile(my.data, probs = seq(0.1,0.9,0.1))
my.data_sorted = sort(my.data)
print("naive approach:")
my.data_sorted[seq(100, 900, 100)]
res <- list()
print("how it is done in r:")
res <- c()
for(i in seq(100, 900, 100)) {
  res<-c(res,((my.data_sorted[i])*(i/1000)+((my.data_sorted[i+1])*((1000-i)/1000))))
}
res

```


## 5. Sample mean and sample standard error vs the theoretical ones 

### What is the theoretical expected value of the random variable considered? What is the variance? Compare them to the sample values; explain the difference in the variance

Theoretical part using calculations from ***section 2***,

Expected value: $E(Z) = \frac{\mu_1 + \mu_2}{2} = 4$

Variance: $Var(Z) = ... = \frac{1}{2}(100 + 9 + 4 + 100) - 16 = 90.5$

```{r}
sample_variance <- function(x, s_mean) {
    return(sum((x - s_mean)^2)/(length(x)-1))
}
standard_error <- function(x, s_mean) {
    return(sqrt(sample_variance(x, s_mean)))
}
# sample mean and sample variance
s_mean <- mean(my.data)
s_variance <- sample_variance(my.data, s_mean)
s_s_error <- standard_error(my.data, s_mean)
cat("Sample mean\t\t=\t", s_mean, "\n")
cat("Sample variance\t\t=\t", s_variance, "\n")
cat("Sample standard error\t=\t", s_s_error, "\n")

cat("Exp. value and s. mean difference\t=\t", abs(4-s_mean), "\n")
cat("Variance and s. variance difference\t=\t", abs(90.5-s_variance), "\n")
```


## 6. k-sigma rule

### Calculate the fraction of the data that are within $k\sigma$ of the sample mean for $k=1,2,3$. Do we get the result expected? Why or why not?

```{r}
m <- mean(my.data); 
print(m)
s <- sd(my.data)
print("For 1 sigma:")
mean(abs(my.data-m)< s) # should be 0.66
print("For 2 sigma:")
mean(abs(my.data-m)< 2*s) # should be 0.95
print("For 3 sigma:")
mean(abs(my.data-m)< 3*s) # should be 0.997
```

