---
title: 'Seminar 10: Descriptive statistics'
output: html_notebook
---

## 1. Data (Id is the average of the member's no. in the P&S list)

### Task: Generate your data 

This is a sample of size $n=1000$ combined from two samples of size $500$ from the normal distributions $N(\mu_k,\sigma_k^2)$ with $\mu_1 = Id$, $\sigma_1 = 3$ and $\mu_2 = -2$, $\sigma_2 = Id$

Hint: you can use $\texttt{mean = c}(\mu_1,\mu_2)$ and $\texttt{sd = c}(\sigma_1,\sigma_2)$
 

```{r}
mu_1 <- 10
mu_2 <- -2
sd_1 <- 3
sd_2 <- 10
n <- 10000000
mean <- c(mu_1, mu_2)
sd <- c(sd_1, sd_2)

my.data <- rnorm(n, mean, sd)
summary(my.data)
```

## 2. Visualise the data

### Task: Draw the histogram of the data, empirical density, and empirical cdf. Comment on whether the data are close to a normal one

Let $X$ ~ $N(\mu_1, \sigma_1^2)$, $Y$ ~ $N(\mu_2, \sigma_2^2)$. Then, combining these two samples, the expected value is going to be: $E(Z) = \frac{\mu_1 + \mu_2}{2} = 4$.

$Var(Z) = E(Z^2) - {E(Z)}^2  = E(Z^2) - 16 = \frac{1}{2}(E(X^2) + E(Y^2)) - 16 = \frac{1}{2}(\mu_1^2 + \sigma_1^2 + \mu_2^2 + \sigma_2^2) - 16 = \frac{1}{2}(100 + 9 + 4 + 100) - 16 = 90.5$

```{r}
x <- my.data
h <- hist(x, plot=FALSE)
plot(h, col="grey")
xlines <-seq(min(h$breaks),max(h$breaks),length.out=1000)
lines(x = xlines,y=dnorm(xlines, 4, sqrt(90.5)) *length(x)*diff(h$breaks)[1])


# superimpose empirical density

# superimpose normal density

```

### Plot ecdf

```{r} 
# plot ecdf

# superimpose cdf for standard normal

# calculate maximal difference 

```

## 3. Skewness and kurtosis of the data

**Skewness** is defined as
$$
  \mathsf{E} \left[\left({\frac {X-\mu }{\sigma }}\right)^{3}\right]={\frac {\mu _{3}}{\sigma ^{3}}}={\frac {\mathsf{E} \left[(X-\mu )^{3}\right]}{\ \ \ (\mathsf{E} \left[(X-\mu )^{2}\right])^{3/2}}}
$$

Positive skewness means the data have longer right tail and then the mean value $\mathsf{E}(X)$ is greater than the median

**Kurtosis** is defined as 
$$
  \mathsf{E} \left[\left({\frac {X-\mu }{\sigma }}\right)^{4}\right]={\frac {\mu _{4}}{\sigma ^{4}}}={\frac {\mathsf{E} \left[(X-\mu )^{4}\right]}{\ \ \ (\mathsf{E} \left[(X-\mu )^{2}\right])^{2}}}
$$

For a standard normal distribution, the kurtosis is $3$. If we get a larger value, then more data are concentrated ner the mean, and the empirical density is more steep


```{r} 
# your code here
k_central_moment <- function(num){
  return (sum((my.data - mean(my.data))^(num))/n)
}
# skewness
third_central_moment = k_central_moment(3)
sec_central_moment = k_central_moment(2)
skewness = third_central_moment/sec_central_moment^(3/2)
skewness
# kurtosis
four_central_moment = k_central_moment(4)
kurtosis = four_central_moment/sec_central_moment^2
kurtosis
```

## 4. Percentiles 

### For q= 0.1,...,0.9 calculate sample q-percentiles manually (i.e. by sorting the data and picking the corresponding values) and by using percentile

```{r} 
quantile(my.data, probs = seq(0.1,0.9,0.1))
my.data_sorted = sort(my.data)
print("naive approach:")
my.data_sorted[seq(100, 900, 100)]
res <- list()
print("how it is done in r:")
res <- c()
for(i in seq(100, 900, 100)) {
  res<-c(res,((my.data_sorted[i])*(i/1000)+((my.data_sorted[i+1])*((1000-i)/1000))))
}
res

```


## 5. Sample mean and sample standard error vs the theoretical ones 

### What is the theoretical expected value of the random variable considered? What is the variance? Compare them to the sample values; explain the difference in the variance

```{r}
# your code here
# theoretical expected value and variance

# sample mean and sample variance

```


## 6. k-sigma rule

### Calculate the fraction of the data that are within $k\sigma$ of the sample mean for $k=1,2,3$. Do we get the result expected? Why or why not?

```{r}
# your code here

```

